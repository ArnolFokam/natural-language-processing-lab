[
  {"id":"abdulmuminSeparatingGrainsChaff2022","abstract":"We participated in the WMT 2022 Large-Scale Machine Translation Evaluation for the African Languages Shared Task. This work describes our approach, which is based on filtering the given noisy data using a sentence-pair classifier that was built by fine-tuning a pre-trained language model. To train the classifier, we obtain positive samples (i.e. high-quality parallel sentences) from a gold-standard curated dataset and extract negative samples (i.e. low-quality parallel sentences) from automatically aligned parallel data by choosing sentences with low alignment scores. Our final machine translation model was then trained on filtered data, instead of the entire noisy dataset. We empirically validate our approach by evaluating on two common datasets and show that data filtering generally improves overall translation quality, in some cases even significantly.","accessed":{"date-parts":[[2022,11,29]]},"author":[{"family":"Abdulmumin","given":"Idris"},{"family":"Beukman","given":"Michael"},{"family":"Alabi","given":"Jesujoba O."},{"family":"Emezue","given":"Chris"},{"family":"Asiko","given":"Everlyn"},{"family":"Adewumi","given":"Tosin"},{"family":"Muhammad","given":"Shamsuddeen Hassan"},{"family":"Adeyemi","given":"Mofetoluwa"},{"family":"Yousuf","given":"Oreen"},{"family":"Singh","given":"Sahib"},{"family":"Gwadabe","given":"Tajuddeen Rabiu"}],"citation-key":"abdulmuminSeparatingGrainsChaff2022","issued":{"date-parts":[[2022,10,20]]},"number":"arXiv:2210.10692","publisher":"arXiv","source":"arXiv.org","title":"Separating Grains from the Chaff: Using Data Filtering to Improve Multilingual Translation for Low-Resourced African Languages","title-short":"Separating Grains from the Chaff","type":"article","URL":"http://arxiv.org/abs/2210.10692"},
  {"id":"adelaniFewThousandTranslations2022","accessed":{"date-parts":[[2022,11,29]]},"author":[{"family":"Adelani","given":"David"},{"family":"Alabi","given":"Jesujoba"},{"family":"Fan","given":"Angela"},{"family":"Kreutzer","given":"Julia"},{"family":"Shen","given":"Xiaoyu"},{"family":"Reid","given":"Machel"},{"family":"Ruiter","given":"Dana"},{"family":"Klakow","given":"Dietrich"},{"family":"Nabende","given":"Peter"},{"family":"Chang","given":"Ernie"},{"family":"Gwadabe","given":"Tajuddeen"},{"family":"Sackey","given":"Freshia"},{"family":"Dossou","given":"Bonaventure F. P."},{"family":"Emezue","given":"Chris"},{"family":"Leong","given":"Colin"},{"family":"Beukman","given":"Michael"},{"family":"Muhammad","given":"Shamsuddeen"},{"family":"Jarso","given":"Guyo"},{"family":"Yousuf","given":"Oreen"},{"family":"Niyongabo Rubungo","given":"Andre"},{"family":"Hacheme","given":"Gilles"},{"family":"Wairagala","given":"Eric Peter"},{"family":"Nasir","given":"Muhammad Umair"},{"family":"Ajibade","given":"Benjamin"},{"family":"Ajayi","given":"Tunde"},{"family":"Gitau","given":"Yvonne"},{"family":"Abbott","given":"Jade"},{"family":"Ahmed","given":"Mohamed"},{"family":"Ochieng","given":"Millicent"},{"family":"Aremu","given":"Anuoluwapo"},{"family":"Ogayo","given":"Perez"},{"family":"Mukiibi","given":"Jonathan"},{"family":"Ouoba Kabore","given":"Fatoumata"},{"family":"Kalipe","given":"Godson"},{"family":"Mbaye","given":"Derguene"},{"family":"Tapo","given":"Allahsera Auguste"},{"family":"Memdjokam Koagne","given":"Victoire"},{"family":"Munkoh-Buabeng","given":"Edwin"},{"family":"Wagner","given":"Valencia"},{"family":"Abdulmumin","given":"Idris"},{"family":"Awokoya","given":"Ayodele"},{"family":"Buzaaba","given":"Happy"},{"family":"Sibanda","given":"Blessing"},{"family":"Bukula","given":"Andiswa"},{"family":"Manthalu","given":"Sam"}],"citation-key":"adelaniFewThousandTranslations2022","container-title":"Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies","DOI":"10.18653/v1/2022.naacl-main.223","event-place":"Seattle, United States","event-title":"Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies","issued":{"date-parts":[[2022]]},"language":"en","page":"3053-3070","publisher":"Association for Computational Linguistics","publisher-place":"Seattle, United States","source":"DOI.org (Crossref)","title":"A Few Thousand Translations Go a Long Way! Leveraging Pre-trained Models for African News Translation","type":"paper-conference","URL":"https://aclanthology.org/2022.naacl-main.223"},
  {"id":"alabiAdaptingPretrainedLanguage2022","abstract":"Multilingual pre-trained language models (PLMs) have demonstrated impressive performance on several downstream tasks for both high-resourced and low-resourced languages. However, there is still a large performance drop for languages unseen during pre-training, especially African languages. One of the most effective approaches to adapt to a new language is \\textit{language adaptive fine-tuning} (LAFT) -- fine-tuning a multilingual PLM on monolingual texts of a language using the pre-training objective. However, adapting to a target language individually takes a large disk space and limits the cross-lingual transfer abilities of the resulting models because they have been specialized for a single language. In this paper, we perform \\textit{multilingual adaptive fine-tuning} on 17 most-resourced African languages and three other high-resource languages widely spoken on the African continent to encourage cross-lingual transfer learning. To further specialize the multilingual PLM, we removed vocabulary tokens from the embedding layer that corresponds to non-African writing scripts before MAFT, thus reducing the model size by around 50%. Our evaluation on two multilingual PLMs (AfriBERTa and XLM-R) and three NLP tasks (NER, news topic classification, and sentiment classification) shows that our approach is competitive to applying LAFT on individual languages while requiring significantly less disk space. Additionally, we show that our adapted PLM also improves the zero-shot cross-lingual transfer abilities of parameter efficient fine-tuning methods.","accessed":{"date-parts":[[2022,11,29]]},"author":[{"family":"Alabi","given":"Jesujoba O."},{"family":"Adelani","given":"David Ifeoluwa"},{"family":"Mosbach","given":"Marius"},{"family":"Klakow","given":"Dietrich"}],"citation-key":"alabiAdaptingPretrainedLanguage2022","issued":{"date-parts":[[2022,10,18]]},"number":"arXiv:2204.06487","publisher":"arXiv","source":"arXiv.org","title":"Adapting Pre-trained Language Models to African Languages via Multilingual Adaptive Fine-Tuning","type":"article","URL":"http://arxiv.org/abs/2204.06487"},
  {"id":"alabiMassiveVsCurated2020","abstract":"The success of several architectures to learn semantic representations from unannotated text and the availability of these kind of texts in online multilingual resources such as Wikipedia has facilitated the massive and automatic creation of resources for multiple languages. The evaluation of such resources is usually done for the high-resourced languages, where one has a smorgasbord of tasks and test sets to evaluate on. For low-resourced languages, the evaluation is more difficult and normally ignored, with the hope that the impressive capability of deep learning architectures to learn (multilingual) representations in the high-resourced setting holds in the low-resourced setting too. In this paper we focus on two African languages, Yor\\`ub\\'a and Twi, and compare the word embeddings obtained in this way, with word embeddings obtained from curated corpora and a language-dependent processing. We analyse the noise in the publicly available corpora, collect high quality and noisy data for the two languages and quantify the improvements that depend not only on the amount of data but on the quality too. We also use different architectures that learn word representations both from surface forms and characters to further exploit all the available information which showed to be important for these languages. For the evaluation, we manually translate the wordsim-353 word pairs dataset from English into Yor\\`ub\\'a and Twi. As output of the work, we provide corpora, embeddings and the test suits for both languages.","accessed":{"date-parts":[[2022,11,29]]},"author":[{"family":"Alabi","given":"Jesujoba O."},{"family":"Amponsah-Kaakyire","given":"Kwabena"},{"family":"Adelani","given":"David I."},{"family":"Espa√±a-Bonet","given":"Cristina"}],"citation-key":"alabiMassiveVsCurated2020","issued":{"date-parts":[[2020,3,28]]},"number":"arXiv:1912.02481","publisher":"arXiv","source":"arXiv.org","title":"Massive vs. Curated Word Embeddings for Low-Resourced Languages. The Case of Yor\\`ub\\'a and Twi","type":"article","URL":"http://arxiv.org/abs/1912.02481"},
  {"id":"jieBetterModelingIncomplete2019a","accessed":{"date-parts":[[2022,11,20]]},"author":[{"family":"Jie","given":"Zhanming"},{"family":"Xie","given":"Pengjun"},{"family":"Lu","given":"Wei"},{"family":"Ding","given":"Ruixue"},{"family":"Li","given":"Linlin"}],"citation-key":"jieBetterModelingIncomplete2019a","container-title":"Proceedings of the 2019 Conference of the North","DOI":"10.18653/v1/N19-1079","event-place":"Minneapolis, Minnesota","event-title":"Proceedings of the 2019 Conference of the North","issued":{"date-parts":[[2019]]},"language":"en","page":"729-734","publisher":"Association for Computational Linguistics","publisher-place":"Minneapolis, Minnesota","source":"DOI.org (Crossref)","title":"Better Modeling of Incomplete Annotations for Named Entity Recognition","type":"paper-conference","URL":"http://aclweb.org/anthology/N19-1079"},
  {"id":"joshiWordEmbeddingsLow2019","accessed":{"date-parts":[[2022,12,5]]},"author":[{"family":"Joshi","given":"Ishani"},{"family":"Koringa","given":"Purvi"},{"family":"Mitra","given":"Suman"}],"citation-key":"joshiWordEmbeddingsLow2019","container-title":"2019 International Conference on Document Analysis and Recognition Workshops (ICDARW)","DOI":"10.1109/ICDARW.2019.40090","event-place":"Sydney, Australia","event-title":"2019 International Conference on Document Analysis and Recognition Workshops (ICDARW)","ISBN":"978-1-72815-054-3","issued":{"date-parts":[[2019,9]]},"page":"110-115","publisher":"IEEE","publisher-place":"Sydney, Australia","source":"DOI.org (Crossref)","title":"Word Embeddings in Low Resource Gujarati Language","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/8893052/"},
  {"id":"malkinBalancedDataApproach2022","accessed":{"date-parts":[[2022,11,20]]},"author":[{"family":"Malkin","given":"Dan"},{"family":"Limisiewicz","given":"Tomasz"},{"family":"Stanovsky","given":"Gabriel"}],"citation-key":"malkinBalancedDataApproach2022","container-title":"Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies","DOI":"10.18653/v1/2022.naacl-main.361","event-place":"Seattle, United States","event-title":"Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies","issued":{"date-parts":[[2022]]},"language":"en","page":"4903-4915","publisher":"Association for Computational Linguistics","publisher-place":"Seattle, United States","source":"DOI.org (Crossref)","title":"A Balanced Data Approach for Evaluating Cross-Lingual Transfer: Mapping the Linguistic Blood Bank","title-short":"A Balanced Data Approach for Evaluating Cross-Lingual Transfer","type":"paper-conference","URL":"https://aclanthology.org/2022.naacl-main.361"},
  {"id":"nieInadequacyOptimizingAlignment2023","abstract":"Contrastive learning is widely used in areas such as visual representation learning (VRL) and sentence representation learning (SRL). Considering the differences between VRL and SRL in terms of negative sample size and evaluation focus, we believe that the solid findings obtained in VRL may not be entirely carried over to SRL. In this work, we consider the suitability of the decoupled form of contrastive loss, i.e., alignment and uniformity, in SRL. We find a performance gap between sentence representations obtained by jointly optimizing alignment and uniformity on the STS task and those obtained using contrastive loss. Further, we find that the joint optimization of alignment and uniformity during training is prone to overfitting, which does not occur on the contrastive loss. Analyzing them based on the variation of the gradient norms, we find that there is a property of ``gradient dissipation'' in contrastive loss and believe that it is the key to preventing overfitting. We simulate similar \"gradient dissipation\" of contrastive loss on four optimization objectives of two forms, and achieve the same or even better performance than contrastive loss on the STS tasks, confirming our hypothesis.","author":[{"family":"Nie","given":"Zhijie"},{"family":"Zhang","given":"Richong"},{"family":"Mao","given":"Yongyi"}],"citation-key":"nieInadequacyOptimizingAlignment2023","container-title":"The Eleventh International Conference on Learning Representations","issued":{"date-parts":[[2023]]},"title":"On The Inadequacy of Optimizing Alignment and Uniformity in Contrastive Learning of Sentence Representations","type":"paper-conference","URL":"https://openreview.net/forum?id=MxvHVNukama"},
  {"id":"oyewusiNaijaNERComprehensiveNamed2021","abstract":"Most of the common applications of Named Entity Recognition (NER) is on English and other highly available languages. In this work, we present our findings on Named Entity Recognition for 5 Nigerian Languages (Nigerian English, Nigerian Pidgin English, Igbo, Yoruba and Hausa). These languages are considered low-resourced, and very little openly available Natural Language Processing work has been done in most of them. In this work, individual NER models were trained and metrics recorded for each of the languages. We also worked on a combined model that can handle Named Entity Recognition (NER) for any of the five languages. The combined model works well for Named Entity Recognition(NER) on each of the languages and with better performance compared to individual NER models trained specifically on annotated data for the specific language. The aim of this work is to share our learning on how information extraction using Named Entity Recognition can be optimized for the listed Nigerian Languages for inclusion, ease of deployment in production and reusability of models. Models developed during this project are available on GitHub https://git.io/JY0kk and an interactive web app https://nigner.herokuapp.com/.","accessed":{"date-parts":[[2022,12,7]]},"author":[{"family":"Oyewusi","given":"Wuraola Fisayo"},{"family":"Adekanmbi","given":"Olubayo"},{"family":"Okoh","given":"Ifeoma"},{"family":"Onuigwe","given":"Vitus"},{"family":"Salami","given":"Mary Idera"},{"family":"Osakuade","given":"Opeyemi"},{"family":"Ibejih","given":"Sharon"},{"family":"Musa","given":"Usman Abdullahi"}],"citation-key":"oyewusiNaijaNERComprehensiveNamed2021","issued":{"date-parts":[[2021,3,30]]},"number":"arXiv:2105.00810","publisher":"arXiv","source":"arXiv.org","title":"NaijaNER : Comprehensive Named Entity Recognition for 5 Nigerian Languages","title-short":"NaijaNER","type":"article","URL":"http://arxiv.org/abs/2105.00810"},
  {"id":"zhouCloserLookHow2022","accessed":{"date-parts":[[2022,11,20]]},"author":[{"family":"Zhou","given":"Yichu"},{"family":"Srikumar","given":"Vivek"}],"citation-key":"zhouCloserLookHow2022","container-title":"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","DOI":"10.18653/v1/2022.acl-long.75","event-place":"Dublin, Ireland","event-title":"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","issued":{"date-parts":[[2022]]},"language":"en","page":"1046-1061","publisher":"Association for Computational Linguistics","publisher-place":"Dublin, Ireland","source":"DOI.org (Crossref)","title":"A Closer Look at How Fine-tuning Changes BERT","type":"paper-conference","URL":"https://aclanthology.org/2022.acl-long.75"}
]
